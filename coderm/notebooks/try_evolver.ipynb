{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84785d25-8597-4f48-8e41-54ab775c67ed",
   "metadata": {
    "thread": {
     "embedding": {
      "headIndex": 1,
      "headName": "response_encoding",
      "quantizedEmbedding": {
       "0": 238,
       "1": 36,
       "10": 17,
       "11": 128,
       "12": 0,
       "13": 128,
       "14": 127,
       "15": 32,
       "16": 57,
       "17": 128,
       "18": 128,
       "19": 206,
       "2": 128,
       "20": 132,
       "21": 127,
       "22": 127,
       "23": 166,
       "24": 127,
       "25": 127,
       "26": 27,
       "27": 128,
       "28": 127,
       "29": 127,
       "3": 36,
       "30": 139,
       "31": 229,
       "32": 128,
       "33": 49,
       "34": 128,
       "35": 85,
       "36": 128,
       "37": 7,
       "38": 127,
       "39": 127,
       "4": 213,
       "40": 28,
       "41": 244,
       "42": 127,
       "43": 127,
       "44": 136,
       "45": 127,
       "46": 172,
       "47": 54,
       "48": 121,
       "49": 127,
       "5": 80,
       "50": 244,
       "51": 124,
       "52": 128,
       "53": 101,
       "54": 103,
       "55": 38,
       "56": 97,
       "57": 136,
       "58": 228,
       "59": 127,
       "6": 128,
       "60": 128,
       "61": 128,
       "62": 211,
       "63": 170,
       "64": 6,
       "65": 127,
       "66": 45,
       "67": 124,
       "68": 128,
       "69": 127,
       "7": 128,
       "70": 204,
       "71": 128,
       "72": 173,
       "73": 136,
       "74": 127,
       "75": 128,
       "76": 127,
       "77": 99,
       "78": 47,
       "79": 175,
       "8": 77,
       "80": 127,
       "81": 61,
       "82": 127,
       "83": 128,
       "84": 127,
       "85": 162,
       "86": 127,
       "87": 172,
       "88": 251,
       "89": 54,
       "9": 214,
       "90": 208,
       "91": 187,
       "92": 143,
       "93": 128,
       "94": 146,
       "95": 65,
       "96": 127,
       "97": 90,
       "98": 117,
       "99": 23
      }
     },
     "ran": true,
     "user": "assistant"
    }
   },
   "outputs": [],
   "source": [
    "from coderm.prompts import py_prompt_evolve, py_prompt\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14e9122-69e8-4515-ab31-5970901a3167",
   "metadata": {
    "thread": {
     "embedding": {
      "headIndex": 1,
      "headName": "response_encoding",
      "quantizedEmbedding": {
       "0": 104,
       "1": 172,
       "10": 192,
       "11": 128,
       "12": 127,
       "13": 128,
       "14": 127,
       "15": 127,
       "16": 0,
       "17": 138,
       "18": 235,
       "19": 8,
       "2": 226,
       "20": 128,
       "21": 127,
       "22": 127,
       "23": 127,
       "24": 228,
       "25": 73,
       "26": 209,
       "27": 128,
       "28": 127,
       "29": 127,
       "3": 127,
       "30": 55,
       "31": 174,
       "32": 128,
       "33": 86,
       "34": 128,
       "35": 127,
       "36": 47,
       "37": 176,
       "38": 127,
       "39": 25,
       "4": 212,
       "40": 50,
       "41": 48,
       "42": 127,
       "43": 127,
       "44": 138,
       "45": 127,
       "46": 236,
       "47": 127,
       "48": 115,
       "49": 127,
       "5": 24,
       "50": 160,
       "51": 109,
       "52": 250,
       "53": 63,
       "54": 127,
       "55": 20,
       "56": 127,
       "57": 186,
       "58": 181,
       "59": 127,
       "6": 128,
       "60": 128,
       "61": 128,
       "62": 127,
       "63": 186,
       "64": 216,
       "65": 127,
       "66": 213,
       "67": 75,
       "68": 128,
       "69": 106,
       "7": 128,
       "70": 217,
       "71": 131,
       "72": 217,
       "73": 210,
       "74": 65,
       "75": 185,
       "76": 127,
       "77": 61,
       "78": 110,
       "79": 168,
       "8": 127,
       "80": 86,
       "81": 252,
       "82": 127,
       "83": 219,
       "84": 218,
       "85": 87,
       "86": 127,
       "87": 198,
       "88": 154,
       "89": 78,
       "9": 216,
       "90": 138,
       "91": 215,
       "92": 169,
       "93": 128,
       "94": 163,
       "95": 119,
       "96": 127,
       "97": 116,
       "98": 4,
       "99": 180
      }
     },
     "ran": true,
     "user": "assistant"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/federicocassano/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78a573c3ee04743a49c1ad405cce8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/833 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:17 config.py:1130] Casting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:17 config.py:1151] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:17 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='federico-staging/train_sc2_15b_evolver', speculative_config=None, tokenizer='federico-staging/train_sc2_15b_evolver', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=federico-staging/train_sc2_15b_evolver)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:19 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:19 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:26 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:26 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:31:26 weight_utils.py:207] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42776aa8e01e4f849436f6924e4b22b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c45b505db4848bb8bff95b9e041cfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f1efc0582a4c33b4380449d5bc0c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f047861293e44296b78bd033df30ae98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18175966f37441f6a332ad5e986b9a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd196a61d2a4c4fa776bbda56ca21be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2b7ebf3d77404e941ffd334b2b3029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b4581743c94a3e994027247b343da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1b42e545724a36816b598d9ce07ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0162d740eb840bdaef415613d12b2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edc355e830d492d9f49bf3ffb4d357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f8d8191b9b457981b9e0ea36ed8d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d15925501ce4af79d67dc2aaba99f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00014.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b073a1a5f65649449fd9fa5fc2c6fbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00014.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355ced58a18b4d53b08cd6031a91e55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/52.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:38:49 model_runner.py:146] Loading model weights took 29.7278 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:38:52 gpu_executor.py:83] # GPU blocks: 31464, # CPU blocks: 3276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:38:53 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:38:53 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:38:59 model_runner.py:924] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"federico-staging/train_sc2_15b_evolver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a29d1c2-f67f-4551-b74d-e5c4d2c69476",
   "metadata": {
    "thread": {
     "embedding": {
      "headIndex": 1,
      "headName": "response_encoding",
      "quantizedEmbedding": {
       "0": 95,
       "1": 146,
       "10": 119,
       "11": 7,
       "12": 227,
       "13": 128,
       "14": 127,
       "15": 100,
       "16": 127,
       "17": 132,
       "18": 158,
       "19": 37,
       "2": 29,
       "20": 128,
       "21": 127,
       "22": 104,
       "23": 128,
       "24": 241,
       "25": 107,
       "26": 239,
       "27": 128,
       "28": 124,
       "29": 127,
       "3": 127,
       "30": 106,
       "31": 245,
       "32": 142,
       "33": 127,
       "34": 246,
       "35": 127,
       "36": 127,
       "37": 194,
       "38": 53,
       "39": 254,
       "4": 11,
       "40": 127,
       "41": 223,
       "42": 127,
       "43": 121,
       "44": 171,
       "45": 126,
       "46": 19,
       "47": 128,
       "48": 85,
       "49": 127,
       "5": 52,
       "50": 128,
       "51": 122,
       "52": 238,
       "53": 7,
       "54": 210,
       "55": 75,
       "56": 70,
       "57": 2,
       "58": 127,
       "59": 127,
       "6": 128,
       "60": 128,
       "61": 62,
       "62": 127,
       "63": 128,
       "64": 165,
       "65": 83,
       "66": 127,
       "67": 214,
       "68": 128,
       "69": 127,
       "7": 128,
       "70": 23,
       "71": 128,
       "72": 70,
       "73": 169,
       "74": 223,
       "75": 128,
       "76": 127,
       "77": 2,
       "78": 166,
       "79": 4,
       "8": 22,
       "80": 38,
       "81": 206,
       "82": 18,
       "83": 247,
       "84": 144,
       "85": 127,
       "86": 127,
       "87": 128,
       "88": 5,
       "89": 69,
       "9": 126,
       "90": 169,
       "91": 0,
       "92": 202,
       "93": 128,
       "94": 128,
       "95": 51,
       "96": 127,
       "97": 58,
       "98": 222,
       "99": 75
      }
     },
     "ran": true,
     "user": "assistant"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processed prompts:   0%|                                                                                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s, Generation Speed: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.02s/it, Generation Speed: 67.19 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.02s/it, Generation Speed: 67.19 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Read the number of queries\n",
      "Q = int(input())\n",
      "\n",
      "# Initialize an empty list to store the sequence A\n",
      "A = []\n",
      "\n",
      "# Iterate through each query\n",
      "for i in range(Q):\n",
      "    # Read the query type and its arguments\n",
      "    (a, b) = map(int, input().split())\n",
      "\n",
      "    # If the query type is 1, append the argument to the end of the sequence A\n",
      "    if a == 1:\n",
      "        A.append(b)\n",
      "\n",
      "    # If the query type is 2, print the k-th value from the end of the sequence A\n",
      "    else:\n",
      "        print(A[-b])\n",
      "# ==== EVOLVED CODE ====\n",
      "\n",
      "# Read the number of queries\n",
      "Q = int(input())\n",
      "\n",
      "# Initialize an empty list to store the sequence A\n",
      "A = []\n",
      "\n",
      "# Initialize an empty list to store the answers to the second type queries\n",
      "ans = []\n",
      "\n",
      "# Iterate through each query\n",
      "for i in range(Q):\n",
      "    # Read the query type and its arguments\n",
      "    (q, x) = map(int, input().split())\n",
      "\n",
      "    # If the query type is 1, append the argument to the end of the sequence A\n",
      "    if q == 1:\n",
      "        A.append(x)\n",
      "    # If the query type is 2, find the k-th value from the end of A and append it to the answer list\n",
      "    else:\n",
      "        ans.append(A[-x])\n",
      "\n",
      "# Print the answers to the second type queries, separated by newlines\n",
      "print(*ans, sep='\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"You have an empty sequence A. There are Q queries given, and you need to process them in the order they are given.\\nThe queries are of the following two types:\\n\\n- 1 x: Append x to the end of A.\\n- 2 k: Find the k-th value from the end of A. It is guaranteed that the length of A is at least k when this query is given.\\n\\nInput\\n\\nThe input is given from Standard Input in the following format:\\nQ\\n\\\\mathrm{query}_1\\n\\\\mathrm{query}_2\\n\\\\vdots\\n\\\\mathrm{query}_Q\\n\\nEach query is in one of the following two formats:\\n1 x\\n\\n2 k\\n\\nOutput\\n\\nPrint q lines, where q is the number of queries of the second type.\\nThe i-th line should contain the answer to the i-th such query.\\n\\nConstraints\\n\\n\\n- 1 \\\\leq Q \\\\leq 100\\n- In the first type of query, x is an integer satisfying 1 \\\\leq x \\\\leq 10^9.\\n- In the second type of query, k is a positive integer not greater than the current length of sequence A.\\n\\nSample Input 1\\n\\n5\\n1 20\\n1 30\\n2 1\\n1 40\\n2 3\\n\\nSample Output 1\\n\\n30\\n20\\n\\n\\n- Initially, A is empty.\\n- The first query appends 20 to the end of A, making A=(20).\\n- The second query appends 30 to the end of A, making A=(20,30).\\n- The answer to the third query is 30, which is the 1-st value from the end of A=(20,30).\\n- The fourth query appends 40 to the end of A, making A=(20,30,40).\\n- The answer to the fifth query is 20, which is the 3-rd value from the end of A=(20,30,40).\"\n",
    "code = \"\\n\\n# Read the input values for the volumes of the regions\\n(v1, v2, v3) = map(int, input().split())\\n\\n# Check if the sum of the volumes is not equal to 343\\nif v1 + v2 + v3 != 343:\\n    # If not, print \\\"No\\\" and exit the program\\n    print('No')\\n    exit()\\n\\n# Check if the volume of the region contained in exactly one cube is not equal to 7\\nif v1 != 7:\\n    # If not, print \\\"No\\\" and exit the program\\n    print('No')\\n    exit()\\n\\n# Check if the volume of the region contained in exactly two cubes is not equal to 84\\nif v2 != 84:\\n    # If not, print \\\"No\\\" and exit the program\\n    print('No')\\n    exit()\\n\\n# If all conditions are satisfied, print \\\"Yes\\\" and the coordinates of the cubes\\nprint('Yes')\\nprint(0, 0, 0, 0, 6, 0, 6, 0, 0)\"\n",
    "p = py_prompt(q)\n",
    "#p = py_prompt_evolve(p)\n",
    "print(model.generate([p], SamplingParams(temperature=0.2, max_tokens=4096))[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "thread": {
   "kernelId": "963c9edd-cdd9-48a1-8c49-fcd8e3f71637",
   "sessionId": "8b1fbb4c-019c-4c82-b99a-ca6d5d282e58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
